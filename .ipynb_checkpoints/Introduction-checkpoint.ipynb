{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proyecto b√°sico de Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Importacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - ETA:  - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.23247   0.        8.14      0.        0.538     6.142    91.7\n",
      "   3.9769    4.      307.       21.      396.9      18.72   ] 15.2\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
      "  0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
      "  0.8252202 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# first we fit the scaler on the training dataset\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# then we call the transform method to scale both the training and testing data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# a sample output\n",
    "print(X_train_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation\n",
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(8, activation='relu', input_shape=[X_train.shape[1]]))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 581.0734 - mae: 21.97 - 0s 22ms/step - loss: 565.8102 - mae: 21.9063 - val_loss: 625.5166 - val_mae: 23.1906\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 600.3062 - mae: 22.74 - 0s 7ms/step - loss: 551.4401 - mae: 21.5119 - val_loss: 612.3036 - val_mae: 22.8369\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 460.8735 - mae: 20.10 - ETA: 0s - loss: 521.2177 - mae: 20.84 - 0s 9ms/step - loss: 538.4099 - mae: 21.1550 - val_loss: 599.9243 - val_mae: 22.5017\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 565.5783 - mae: 21.72 - ETA: 0s - loss: 541.2887 - mae: 21.17 - 0s 9ms/step - loss: 526.0162 - mae: 20.7849 - val_loss: 587.2846 - val_mae: 22.1451\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 550.5958 - mae: 21.62 - ETA: 0s - loss: 499.0405 - mae: 20.17 - 0s 9ms/step - loss: 512.9453 - mae: 20.4002 - val_loss: 573.5936 - val_mae: 21.7631\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 449.3113 - mae: 18.72 - ETA: 0s - loss: 494.5043 - mae: 20.05 - 0s 9ms/step - loss: 499.3794 - mae: 20.0035 - val_loss: 559.8624 - val_mae: 21.3842\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 470.0764 - mae: 18.86 - ETA: 0s - loss: 486.5674 - mae: 19.38 - 0s 10ms/step - loss: 485.3428 - mae: 19.5892 - val_loss: 545.4630 - val_mae: 20.9865\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 414.3967 - mae: 17.53 - 0s 4ms/step - loss: 470.5971 - mae: 19.1417 - val_loss: 529.9956 - val_mae: 20.5362\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 607.8812 - mae: 21.77 - 0s 4ms/step - loss: 455.1540 - mae: 18.6629 - val_loss: 514.6489 - val_mae: 20.0883\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 469.0725 - mae: 19.10 - ETA: 0s - loss: 430.4382 - mae: 18.08 - 0s 8ms/step - loss: 439.6299 - mae: 18.2362 - val_loss: 498.6861 - val_mae: 19.6269\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 537.1819 - mae: 18.94 - ETA: 0s - loss: 550.2336 - mae: 19.95 - 0s 9ms/step - loss: 423.7660 - mae: 17.7700 - val_loss: 482.2268 - val_mae: 19.2001\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 301.4170 - mae: 15.48 - ETA: 0s - loss: 425.6348 - mae: 17.90 - 0s 9ms/step - loss: 407.1652 - mae: 17.3020 - val_loss: 464.7715 - val_mae: 18.7417\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 451.8167 - mae: 18.42 - ETA: 0s - loss: 432.1288 - mae: 18.06 - 0s 9ms/step - loss: 389.8720 - mae: 16.7885 - val_loss: 446.7786 - val_mae: 18.2532\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 453.7419 - mae: 18.60 - ETA: 0s - loss: 391.3485 - mae: 17.01 - 0s 9ms/step - loss: 372.0262 - mae: 16.2998 - val_loss: 428.9212 - val_mae: 17.7696\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 327.4780 - mae: 15.95 - 0s 4ms/step - loss: 354.2722 - mae: 15.7970 - val_loss: 412.1497 - val_mae: 17.3084\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 409.1121 - mae: 17.16 - ETA: 0s - loss: 353.5851 - mae: 16.10 - 0s 9ms/step - loss: 337.3591 - mae: 15.3372 - val_loss: 392.9876 - val_mae: 16.7680\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 291.6069 - mae: 15.34 - 0s 4ms/step - loss: 318.7695 - mae: 14.8021 - val_loss: 375.6311 - val_mae: 16.2712\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 458.1631 - mae: 18.07 - ETA: 0s - loss: 305.3970 - mae: 14.32 - 0s 8ms/step - loss: 300.9667 - mae: 14.3084 - val_loss: 354.2136 - val_mae: 15.6563\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 199.5176 - mae: 12.61 - ETA: 0s - loss: 301.8812 - mae: 14.21 - 0s 9ms/step - loss: 280.9148 - mae: 13.7349 - val_loss: 335.9102 - val_mae: 15.1265\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 311.7111 - mae: 14.35 - ETA: 0s - loss: 280.5984 - mae: 13.72 - 0s 8ms/step - loss: 262.2831 - mae: 13.1878 - val_loss: 316.8380 - val_mae: 14.5561\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 265.7033 - mae: 14.04 - ETA: 0s - loss: 264.1526 - mae: 13.13 - 0s 10ms/step - loss: 243.9062 - mae: 12.6220 - val_loss: 296.1527 - val_mae: 13.9256\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 248.9300 - mae: 11.67 - ETA: 0s - loss: 237.4420 - mae: 12.27 - 0s 9ms/step - loss: 223.6147 - mae: 11.9613 - val_loss: 273.9897 - val_mae: 13.2093\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 183.2267 - mae: 10.74 - ETA: 0s - loss: 186.7098 - mae: 10.79 - 0s 8ms/step - loss: 203.5622 - mae: 11.2919 - val_loss: 256.1163 - val_mae: 12.5859\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 207.8242 - mae: 10.54 - ETA: 0s - loss: 187.3217 - mae: 10.55 - 0s 9ms/step - loss: 186.8911 - mae: 10.6492 - val_loss: 236.5660 - val_mae: 11.8565\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 238.1474 - mae: 11.46 - 0s 5ms/step - loss: 169.0431 - mae: 9.9791 - val_loss: 217.7677 - val_mae: 11.1182\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 134.0107 - mae: 9.989 - 0s 4ms/step - loss: 152.6598 - mae: 9.2912 - val_loss: 201.3103 - val_mae: 10.4639\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 122.9877 - mae: 7.801 - ETA: 0s - loss: 116.3920 - mae: 8.230 - 0s 8ms/step - loss: 138.3157 - mae: 8.6812 - val_loss: 184.0243 - val_mae: 9.7431\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 151.1289 - mae: 8.935 - ETA: 0s - loss: 111.6813 - mae: 8.010 - 0s 9ms/step - loss: 124.6991 - mae: 8.0217 - val_loss: 169.4040 - val_mae: 9.1062\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 153.9374 - mae: 9.130 - ETA: 0s - loss: 112.5154 - mae: 7.665 - 0s 9ms/step - loss: 113.2437 - mae: 7.4742 - val_loss: 156.9013 - val_mae: 8.5708\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 129.3447 - mae: 7.545 - 0s 9ms/step - loss: 104.4429 - mae: 7.0442 - val_loss: 146.5842 - val_mae: 8.1769\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 63.4434 - mae: 5.76 - 0s 10ms/step - loss: 96.7118 - mae: 6.7686 - val_loss: 138.5692 - val_mae: 7.8970\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 58.7385 - mae: 5.61 - 0s 10ms/step - loss: 91.0613 - mae: 6.5382 - val_loss: 130.2811 - val_mae: 7.6521\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 115.1305 - mae: 7.762 - 0s 10ms/step - loss: 85.3621 - mae: 6.3435 - val_loss: 123.2134 - val_mae: 7.4474\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 104.5924 - mae: 7.465 - ETA: 0s - loss: 80.8030 - mae: 6.187 - 0s 9ms/step - loss: 80.7940 - mae: 6.2047 - val_loss: 117.5004 - val_mae: 7.2586\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 49.4753 - mae: 5.72 - ETA: 0s - loss: 72.6212 - mae: 5.89 - 0s 10ms/step - loss: 76.7688 - mae: 6.0632 - val_loss: 112.0927 - val_mae: 7.0781\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 46.2241 - mae: 5.16 - ETA: 0s - loss: 66.9228 - mae: 5.77 - 0s 8ms/step - loss: 72.9100 - mae: 5.9251 - val_loss: 107.2967 - val_mae: 6.9213\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 97.5940 - mae: 6.86 - ETA: 0s - loss: 66.9268 - mae: 5.59 - 0s 14ms/step - loss: 69.7109 - mae: 5.7934 - val_loss: 102.8657 - val_mae: 6.7817\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 49.8083 - mae: 5.17 - 0s 4ms/step - loss: 66.3936 - mae: 5.6592 - val_loss: 98.7141 - val_mae: 6.6211\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 53.1294 - mae: 5.19 - 0s 9ms/step - loss: 63.4441 - mae: 5.5219 - val_loss: 95.1615 - val_mae: 6.4845\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 59.5337 - mae: 5.67 - 0s 10ms/step - loss: 60.8442 - mae: 5.3856 - val_loss: 91.2141 - val_mae: 6.3472\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 48.9072 - mae: 5.09 - 0s 8ms/step - loss: 58.2096 - mae: 5.2476 - val_loss: 87.3317 - val_mae: 6.2455\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 97.6052 - mae: 6.12 - 0s 8ms/step - loss: 55.7573 - mae: 5.1534 - val_loss: 83.7799 - val_mae: 6.1543\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 28.8685 - mae: 4.31 - 0s 8ms/step - loss: 53.1746 - mae: 5.0454 - val_loss: 80.5090 - val_mae: 5.9922\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 28.4156 - mae: 3.91 - 0s 9ms/step - loss: 50.7807 - mae: 4.8931 - val_loss: 76.1185 - val_mae: 5.9588\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 24.3078 - mae: 4.04 - 0s 8ms/step - loss: 48.2302 - mae: 4.8250 - val_loss: 72.6061 - val_mae: 5.8733\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 44.9559 - mae: 4.75 - 0s 5ms/step - loss: 46.0628 - mae: 4.7309 - val_loss: 69.9043 - val_mae: 5.6958\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 21.2323 - mae: 3.65 - 0s 4ms/step - loss: 44.4128 - mae: 4.5956 - val_loss: 66.9610 - val_mae: 5.6141\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 29.8246 - mae: 3.84 - ETA: 0s - loss: 25.5383 - mae: 3.74 - 0s 14ms/step - loss: 42.5111 - mae: 4.4901 - val_loss: 64.5593 - val_mae: 5.5162\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 40.1857 - mae: 4.55 - 0s 4ms/step - loss: 41.1135 - mae: 4.3841 - val_loss: 61.6837 - val_mae: 5.3998\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 32.1175 - mae: 4.41 - 0s 4ms/step - loss: 39.2242 - mae: 4.2852 - val_loss: 58.6027 - val_mae: 5.2943\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 35.1832 - mae: 4.27 - ETA: 0s - loss: 47.1300 - mae: 4.49 - 0s 9ms/step - loss: 37.5640 - mae: 4.1986 - val_loss: 55.7540 - val_mae: 5.2305\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 12.6895 - mae: 2.77 - ETA: 0s - loss: 40.2639 - mae: 4.24 - 0s 8ms/step - loss: 35.9009 - mae: 4.1203 - val_loss: 53.8592 - val_mae: 5.0644\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 36.5133 - mae: 4.35 - ETA: 0s - loss: 43.0235 - mae: 4.33 - 0s 10ms/step - loss: 34.8855 - mae: 4.0336 - val_loss: 52.1224 - val_mae: 4.9710\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 20.4063 - mae: 3.52 - ETA: 0s - loss: 34.1422 - mae: 3.98 - 0s 8ms/step - loss: 33.7709 - mae: 3.9472 - val_loss: 50.1560 - val_mae: 4.9388\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 17.9262 - mae: 3.42 - ETA: 0s - loss: 31.9058 - mae: 3.87 - 0s 9ms/step - loss: 32.6583 - mae: 3.8935 - val_loss: 48.2763 - val_mae: 4.9305\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 16.3221 - mae: 2.98 - ETA: 0s - loss: 33.6115 - mae: 3.94 - 0s 9ms/step - loss: 31.7633 - mae: 3.8734 - val_loss: 46.3496 - val_mae: 4.7719\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 23.7485 - mae: 3.90 - ETA: 0s - loss: 37.2590 - mae: 4.17 - 0s 9ms/step - loss: 30.8657 - mae: 3.7934 - val_loss: 44.7588 - val_mae: 4.6831\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 43.6765 - mae: 4.03 - ETA: 0s - loss: 32.4610 - mae: 3.84 - 0s 10ms/step - loss: 29.9463 - mae: 3.7394 - val_loss: 43.0350 - val_mae: 4.6414\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 25.6179 - mae: 3.68 - ETA: 0s - loss: 30.6529 - mae: 3.72 - 0s 8ms/step - loss: 29.1188 - mae: 3.7129 - val_loss: 41.5287 - val_mae: 4.5471\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 24.3911 - mae: 3.58 - ETA: 0s - loss: 27.3007 - mae: 3.52 - 0s 8ms/step - loss: 28.3260 - mae: 3.6485 - val_loss: 39.9372 - val_mae: 4.4753\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 15.7852 - mae: 3.04 - ETA: 0s - loss: 24.3790 - mae: 3.53 - 0s 9ms/step - loss: 27.5539 - mae: 3.6005 - val_loss: 38.3935 - val_mae: 4.4119\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 26.4578 - mae: 3.88 - ETA: 0s - loss: 26.7604 - mae: 3.55 - 0s 9ms/step - loss: 26.7794 - mae: 3.5656 - val_loss: 37.1887 - val_mae: 4.3356\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 23.4681 - mae: 3.89 - ETA: 0s - loss: 26.9337 - mae: 3.58 - 0s 9ms/step - loss: 26.1525 - mae: 3.5216 - val_loss: 36.1174 - val_mae: 4.2690\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 23.3287 - mae: 3.42 - ETA: 0s - loss: 25.7786 - mae: 3.51 - 0s 8ms/step - loss: 25.6250 - mae: 3.4916 - val_loss: 34.8297 - val_mae: 4.2352\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 11.6426 - mae: 2.62 - ETA: 0s - loss: 22.0999 - mae: 3.25 - 0s 10ms/step - loss: 25.0410 - mae: 3.4604 - val_loss: 33.8621 - val_mae: 4.1868\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 14.7348 - mae: 3.04 - ETA: 0s - loss: 23.1032 - mae: 3.37 - 0s 8ms/step - loss: 24.5378 - mae: 3.4413 - val_loss: 32.8321 - val_mae: 4.1275\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 22.9379 - mae: 3.26 - ETA: 0s - loss: 22.2682 - mae: 3.34 - 0s 9ms/step - loss: 23.9651 - mae: 3.4011 - val_loss: 31.8694 - val_mae: 4.0811\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 15.5502 - mae: 3.10 - ETA: 0s - loss: 24.2349 - mae: 3.42 - 0s 8ms/step - loss: 23.5627 - mae: 3.3806 - val_loss: 30.9852 - val_mae: 4.0221\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 14.1273 - mae: 2.80 - ETA: 0s - loss: 22.9299 - mae: 3.33 - 0s 9ms/step - loss: 22.9890 - mae: 3.3391 - val_loss: 29.7537 - val_mae: 3.9972\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 10.8509 - mae: 2.72 - 0s 8ms/step - loss: 22.4755 - mae: 3.3224 - val_loss: 29.1625 - val_mae: 3.9642\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 14.0711 - mae: 2.94 - 0s 8ms/step - loss: 22.1348 - mae: 3.2963 - val_loss: 28.4769 - val_mae: 3.9217\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 18.5758 - mae: 2.79 - 0s 9ms/step - loss: 21.7048 - mae: 3.2748 - val_loss: 27.6846 - val_mae: 3.8562\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 19.6287 - mae: 3.16 - 0s 8ms/step - loss: 21.2990 - mae: 3.2317 - val_loss: 26.8604 - val_mae: 3.7988\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 48.1899 - mae: 4.22 - 0s 8ms/step - loss: 20.8462 - mae: 3.2019 - val_loss: 26.1735 - val_mae: 3.7081\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 12.9546 - mae: 2.75 - 0s 9ms/step - loss: 20.4660 - mae: 3.1645 - val_loss: 25.6400 - val_mae: 3.7170\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 16.7821 - mae: 2.75 - 0s 8ms/step - loss: 20.0340 - mae: 3.1376 - val_loss: 25.1042 - val_mae: 3.6615\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 8.8198 - mae: 2.453 - 0s 9ms/step - loss: 19.6313 - mae: 3.1112 - val_loss: 24.6361 - val_mae: 3.6327\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 17.9401 - mae: 3.26 - 0s 4ms/step - loss: 19.4780 - mae: 3.0904 - val_loss: 24.1359 - val_mae: 3.6574\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 17.5593 - mae: 3.52 - ETA: 0s - loss: 20.4362 - mae: 3.26 - 0s 8ms/step - loss: 18.9675 - mae: 3.0585 - val_loss: 23.4846 - val_mae: 3.6182\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 23.0440 - mae: 3.55 - 0s 5ms/step - loss: 18.6875 - mae: 3.0439 - val_loss: 22.8783 - val_mae: 3.5093\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 11.6425 - mae: 2.54 - 0s 4ms/step - loss: 18.3346 - mae: 3.0124 - val_loss: 22.4878 - val_mae: 3.5155\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 29.2675 - mae: 2.98 - ETA: 0s - loss: 31.2315 - mae: 3.34 - 0s 14ms/step - loss: 18.1202 - mae: 2.9922 - val_loss: 22.0863 - val_mae: 3.4253\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 9.4555 - mae: 2.453 - 0s 4ms/step - loss: 17.8281 - mae: 2.9605 - val_loss: 21.8785 - val_mae: 3.4935\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 28.5266 - mae: 3.65 - ETA: 0s - loss: 18.3083 - mae: 2.88 - 0s 9ms/step - loss: 17.5035 - mae: 2.9425 - val_loss: 21.4635 - val_mae: 3.4058\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 31.8440 - mae: 3.44 - 0s 4ms/step - loss: 17.1816 - mae: 2.9030 - val_loss: 21.0139 - val_mae: 3.3873\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 19.3202 - mae: 3.18 - 0s 4ms/step - loss: 16.9268 - mae: 2.8847 - val_loss: 20.9034 - val_mae: 3.3779\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 13.5146 - mae: 2.37 - ETA: 0s - loss: 12.9255 - mae: 2.62 - 0s 8ms/step - loss: 16.6152 - mae: 2.8507 - val_loss: 20.7817 - val_mae: 3.4458\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 8.6103 - mae: 2.381 - ETA: 0s - loss: 17.8028 - mae: 2.92 - 0s 9ms/step - loss: 16.3972 - mae: 2.8411 - val_loss: 20.5818 - val_mae: 3.4461\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 13.1326 - mae: 2.80 - ETA: 0s - loss: 13.3577 - mae: 2.79 - 0s 8ms/step - loss: 16.1210 - mae: 2.8299 - val_loss: 19.8162 - val_mae: 3.2737\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 21.3382 - mae: 3.02 - ETA: 0s - loss: 16.5565 - mae: 2.81 - 0s 9ms/step - loss: 15.8797 - mae: 2.8046 - val_loss: 19.8020 - val_mae: 3.3688\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 20.7148 - mae: 3.44 - ETA: 0s - loss: 15.8945 - mae: 2.85 - 0s 9ms/step - loss: 15.6892 - mae: 2.7911 - val_loss: 19.4724 - val_mae: 3.3168\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 9.9425 - mae: 2.477 - ETA: 0s - loss: 14.1498 - mae: 2.66 - 0s 9ms/step - loss: 15.4765 - mae: 2.7810 - val_loss: 19.1653 - val_mae: 3.2502\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 9.3991 - mae: 2.422 - ETA: 0s - loss: 14.1261 - mae: 2.75 - 0s 10ms/step - loss: 15.3087 - mae: 2.7627 - val_loss: 18.8346 - val_mae: 3.1938\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 19.8061 - mae: 3.28 - ETA: 0s - loss: 15.2976 - mae: 2.75 - 0s 7ms/step - loss: 15.1625 - mae: 2.7385 - val_loss: 18.6476 - val_mae: 3.2010\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 7.3286 - mae: 2.035 - ETA: 0s - loss: 18.1069 - mae: 2.77 - 0s 9ms/step - loss: 15.0462 - mae: 2.7361 - val_loss: 18.4256 - val_mae: 3.2132\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 18.8255 - mae: 2.94 - ETA: 0s - loss: 14.6461 - mae: 2.69 - 0s 8ms/step - loss: 14.7896 - mae: 2.6964 - val_loss: 18.3854 - val_mae: 3.2116\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 8.0138 - mae: 2.147 - ETA: 0s - loss: 15.5425 - mae: 2.75 - 0s 9ms/step - loss: 14.6728 - mae: 2.7028 - val_loss: 18.0875 - val_mae: 3.1527\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 8.4957 - mae: 2.142 - ETA: 0s - loss: 12.7111 - mae: 2.61 - 0s 11ms/step - loss: 14.4424 - mae: 2.6772 - val_loss: 17.9349 - val_mae: 3.1737\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 13.0349 - mae: 2.85 - 0s 6ms/step - loss: 14.2354 - mae: 2.6620 - val_loss: 17.6831 - val_mae: 3.1172\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 8.1069 - mae: 2.136 - 0s 8ms/step - loss: 14.1705 - mae: 2.6274 - val_loss: 17.3504 - val_mae: 3.0487\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
